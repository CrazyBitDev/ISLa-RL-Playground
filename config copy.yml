use_wandb: True
wandb_config:
  project: RL-playground
  entity: matteoingusci01
  tag: VR506254

DRL_methods:

  - name: PPO 
    parameters:
      hidden_layers: 2                        # The number of hidden layers of the neural network
      nodes_hidden_layers: 32                 # The number of neurons in hidden layers of the neural network
      gamma: 0.99                             # Discount factor
      clip: 0.2                               # Clipping parameter
      epochs: 10                              # Number of epochs
      lr_optimizer_pi: 0.001                  # learning rate optimizer policy pi
      lr_optimizer_vf: 0.001                  # learning rate optimizer value funtion
     
    gym_environment: CartPole-v1
    tot_episodes: 3000                        # The number of episodes to run the agent
    seeds_to_test: [0,1,2]



  - name: MCTS 
    parameters:
      step_penality: 0.01                     # Penalty for each step
      hole_score: -1.0                        # Score for falling in a hole (FrozenLake)
  
    gym_environment: FrozenLake
    tot_episodes: 2000                        # The number of episodes to run the agent
    #render_mode: human                       # Render mode (if uncommented the agent will be rendered)
    seeds_to_test: [0,1,2]



  - name: SAC
    parameters:
      hidden_layers_actor: 2                  # The number of hidden layers of the actor neural network
      hidden_layers_critic: 2                 # The number of hidden layers of the critic neural network
      hidden_layers_value: 2                  # The number of hidden layers of the value neural network
      nodes_hidden_layers_actor: 512          # The number of neurons in hidden layers of the neural network
      nodes_hidden_layers_critic: 256         # The number of neurons in hidden layers of the neural network
      nodes_hidden_layers_value: 256          # The number of neurons in hidden layers of the neural network
      lr_actor_optimizer: 0.001               # learning rate optimizer policy pi
      lr_critic_optimizer: 0.001              # learning rate optimizer critic function (Q function)
      lr_value_optimizer: 0.001               # learning rate optimizer value function
      tau: 0.005                              # polyak averaging in soft-update of parameters
      gamma: 0.99                             # Discount factor
     
    gym_environment: LunarLander
    memory_size: 10000                        # The size of the replay buffer
    batch_size: 256                           # The number of samples to take from the replay buffer
    tot_episodes: 250                         # The number of episodes to run the agent
    success_reward_threshold: 200             # 200 is the minimum reward to consider the environment solved (LunarLander)
    seeds_to_test: [0,1,3]



  - name: DDPG
    parameters:
      hidden_layers_actor: 2                  # The number of hidden layers of the actor neural network
      hidden_layers_critic: 2                 # The number of hidden layers of the critic neural network
      nodes_hidden_layers_actor: 128          # The number of neurons in hidden layers of the neural network
      nodes_hidden_layers_critic: 128         # The number of neurons in hidden layers of the neural network
      lr_actor_optimizer: 0.001               # learning rate optimizer policy pi
      lr_critic_optimizer: 0.001              # learning rate optimizer critic function (Q function)
      gamma: 0.99                             # Discount factor
      tau: 0.995                              # polyak averaging in soft-update of parameters
      update_freq: 50                         # The frequency of updating the target network
      n_updates: 100                          # The number of updates to the target network 
      eps_decay: 0.99995                      # The decay of the epsilon noise
  
    gym_environment: TB3
    tot_episodes: 200                         # The number of episodes to run the agent
    seeds_to_test: [0,1,2]
  
